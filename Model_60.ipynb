{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741446b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# PARAMETERS\n",
    "# -----------------------------\n",
    "data_dir = \"./dataset_new\"   # Path to your dataset\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUG_FACTOR = 10\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LOAD FILES & LABELS\n",
    "# -----------------------------\n",
    "classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "class_to_index = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "all_image_paths, all_labels = [], []\n",
    "for cls in classes:\n",
    "    paths = glob(os.path.join(data_dir, cls, \"*\"))\n",
    "    all_image_paths.extend(paths)\n",
    "    all_labels.extend([class_to_index[cls]] * len(paths))\n",
    "\n",
    "all_image_paths = np.array(all_image_paths)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Original dataset size:\", len(all_image_paths))\n",
    "\n",
    "# -----------------------------\n",
    "# PRINT CLASS DISTRIBUTION\n",
    "# -----------------------------\n",
    "for cls, idx in class_to_index.items():\n",
    "    count = np.sum(all_labels == idx)\n",
    "    print(f\"{cls}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# NORMALIZATION FUNCTION \n",
    "# -----------------------------\n",
    "def normalize_image_rgb(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=1, expand_animations=False)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    img = tf.tile(img, [1, 1, 3])\n",
    "    mean = tf.constant([0.485, 0.456, 0.406])\n",
    "    std = tf.constant([0.229, 0.224, 0.225])\n",
    "    img = (img - mean) / std\n",
    "    return img, label\n",
    "\n",
    "# -----------------------------\n",
    "# STRONG AUGMENTATION FUNCTION\n",
    "# -----------------------------\n",
    "def augment_image(img, label):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, 0.9, 1.1)\n",
    "    return img, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4309f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# BUILD DATASET\n",
    "# -----------------------------\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_image_paths, all_labels))\n",
    "dataset = dataset.map(normalize_image_rgb, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SPLIT TRAIN / VAL / TEST (60/20/20)\n",
    "# -----------------------------\n",
    "dataset_size = len(all_image_paths)\n",
    "train_size = int(0.6 * dataset_size)\n",
    "val_size = int(0.2 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size  \n",
    "dataset = dataset.shuffle(dataset_size, reshuffle_each_iteration=False)\n",
    "train_ds = dataset.take(train_size)\n",
    "val_test_ds = dataset.skip(train_size)\n",
    "val_ds = val_test_ds.take(val_size)\n",
    "test_ds = val_test_ds.skip(val_size)\n",
    "\n",
    "print(\"Train size:\", sum(1 for _ in train_ds))\n",
    "print(\"Val size  :\", sum(1 for _ in val_ds))\n",
    "print(\"Test size :\", sum(1 for _ in test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac17dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# AUGMENT TRAINING DATA\n",
    "# -----------------------------\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=AUTOTUNE).repeat(AUG_FACTOR)\n",
    "train_ds = train_ds.shuffle(buffer_size=train_size * AUG_FACTOR)\n",
    "\n",
    "# -----------------------------\n",
    "# BATCH & PREFETCH\n",
    "# -----------------------------\n",
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# MODEL WITH DenseNet121\n",
    "# -----------------------------\n",
    "base_model = DenseNet121(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "base_model.trainable = True  # fine-tune entire base\n",
    "\n",
    "# ADD CUSTOM CLASSIFIER\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)  # binary classification\n",
    "\n",
    "model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# -----------------------------\n",
    "# COMPILE MODEL\n",
    "# -----------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bfd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CALLBACKS\n",
    "# -----------------------------\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN MODEL\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe553d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, split_name=\"Dataset\"):\n",
    "    classes = [\"normal\", \"osteoporosis\"]\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for imgs, lbls in dataset:\n",
    "        preds = model.predict(imgs, verbose=0)\n",
    "        y_true.extend(lbls.numpy())\n",
    "        y_pred.extend((preds > 0.5).astype(\"int32\").flatten())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n{split_name} Accuracy: {acc*100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"{split_name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f\"{split_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Run evaluation\n",
    "# =========================\n",
    "evaluate_model(model, train_ds, split_name=\"Train\")\n",
    "evaluate_model(model, val_ds, split_name=\"Validation\")\n",
    "evaluate_model(model, test_ds, split_name=\"Test\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
